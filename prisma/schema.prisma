// This is your Prisma schema file,
// learn more about it in the docs: https://pris.ly/d/prisma-schema

// Looking for ways to speed up your queries, or scale easily with your serverless or edge functions?
// Try Prisma Accelerate: https://pris.ly/cli/accelerate-init

generator client {
  provider        = "prisma-client-js"
  previewFeatures = ["typedSql"]
  output          = "../src/prisma-client" 
}

datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")
}

enum Status {
  CRAWLING
  SCANNING
  IDLE
  ERRORED
  MISSING
  PENDING
  STOPPED
}

enum Severity {
  HINT
  WARNING
  ERROR
  FATAL
}

enum Confidence {
  HIGH
  MEDIUM
  LOW
  NONE
}

/*
TODO: Once we get partial scanning working,
* Add in the idea of multiple agents hitting the same location
* Add UI to define locations, name them, etc...
* Change reporting to be by location.
model Location {
  id               String   @id @default(cuid(2))
  created_at       DateTime @default(now())
  updated_at       DateTime @default(now()) @updatedAt
  name             String
  root             String
  skip_completed   Boolean
  max_workers      Int
  mem_thresh       Int
  use_history      Boolean
  default_timeout  Int
  agents           AgentsReachingLocation[]
  crawls           Crawl[]
  scans            Scan[]
  @@unique(name: "location_root", [root])
}

model AgentsReachingLocation {
  location      Location @relation(fields: [locationId], references: [id])
  locationId    String
  agent         Agent @relation(fields: [agentId], references: [id])
  agentId       String
  created_at    DateTime @default(now())
  updated_at    DateTime @default(now()) @updatedAt
  @@id([locationId, agentId])
}
*/

model Target {
  id               String   @id @default(cuid(2))
  created_at       DateTime @default(now())
  updated_at       DateTime @default(now()) @updatedAt
  roots            String[]
  skip_completed   Boolean
  max_workers      Int
  mem_thresh       Int
  use_history      Boolean
  default_timeout  Int
  crawls           Crawl[]
  scans            Scan[]
  agentId          String
  agent            Agent @relation(fields: [agentId], references: [id], onDelete: Restrict)
  @@unique(name: "agent_target", [agentId, roots])
}

model Agent {
  id           String   @id
  created_at   DateTime @default(now())
  updated_at   DateTime @default(now()) @updatedAt
  name         String
  location     String
  status       Status   @default(IDLE)
  targets      Target[]
  os           String
  os_version   String
  arch         String
  processor    String
  cores        Int
  logical_cpus Int
  ram_gb       Float
}

model Crawl {
  id                 String   @id @default(cuid(2))
  created_at         DateTime @default(now())
  updated_at         DateTime @default(now()) @updatedAt
  targeted_date      DateTime  
  result_folder      String
  root_path          String
  use_history        Boolean  @default(true)
  file_count         Int
  dir_count          Int
  total_size         BigInt
  scan_size          BigInt
  largest_file_size  BigInt
  largest_file_path  String
  extensions         String[]
  start_time         DateTime @default(now())
  end_time           DateTime
  throughput         Float
  errors             CrawlError[]
  hashes             CrawlHash[]
  unsupported_files  String[]
  target             Target @relation(fields: [targetId], references: [id], onDelete: Restrict)
  targetId           String
  @@index(fields: [updated_at])
  @@index(fields: [result_folder, targetId])
  @@index(fields: [end_time, targetId])
  @@unique(name: "crawl_target_result", [targetId, result_folder])
}

model CrawlError {
  id         String   @id @default(cuid(2))
  created_at DateTime @default(now())
  updated_at DateTime @default(now()) @updatedAt
  error_name String
  error_desc String
  file       String
  crawl      Crawl    @relation(fields: [crawlId], references: [id], onDelete: Cascade)
  crawlId    String
}

model CrawlHash {
  hash       String
  created_at DateTime @default(now())
  updated_at DateTime @default(now()) @updatedAt
  file_paths String[]
  bsize      Int
  format     String
  crawl      Crawl    @relation(fields: [crawlId], references: [id], onDelete: Cascade)
  crawlId    String
  @@id([crawlId, hash])
}

model Scan {
  id              String   @id @default(cuid(2))
  created_at      DateTime @default(now())
  updated_at      DateTime @default(now()) @updatedAt
  targeted_date   DateTime  
  result_folder   String
  root_path       String
  start_time      DateTime @default(now())
  end_time        DateTime?
  matches         Int
  timeouts        Int
  gigs_per_second Float
  errors          ScanError[]
  results         ScanResult[]
  target          Target @relation(fields: [targetId], references: [id], onDelete: Restrict)
  targetId        String
  @@index(fields: [updated_at])
  @@index(fields: [result_folder, targetId])
  @@index(fields: [end_time, targetId])
  @@unique(name: "scan_target_result", [targetId, result_folder])
}

model ScanError {
  id              String   @id
  created_at      DateTime @default(now())
  updated_at      DateTime @default(now()) @updatedAt
  occurred_at     DateTime
  severity        Severity
  error_name      String
  error_desc      String
  file            String
  scan            Scan     @relation(fields: [scanId], references: [id], onDelete: Cascade)
  scanId          String
}

model ScanResult {
  id              String   @id @default(cuid(2))
  created_at      DateTime @default(now())
  updated_at      DateTime @default(now()) @updatedAt
  hash            String
  file_path       String
  mime_type       String
  bsize           Int
  processed       Boolean
  errored         Boolean
  match           String[]
  confidence      Confidence
  scan            Scan     @relation(fields: [scanId], references: [id], onDelete: Cascade)
  scanId          String
  @@index(fields: [confidence])
  @@index(fields: [hash, id, confidence])
  @@index(fields: [updated_at])
}
